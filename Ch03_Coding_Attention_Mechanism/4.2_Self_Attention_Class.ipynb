{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c74a1ffb-f8db-4e70-b2be-84d5a0b20b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "69d23b07-8c20-48da-b3e5-e33295a98755",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.tensor([\n",
    "    [0.43, 0.15, 0.89], # Your      (x^1)\n",
    "    [0.55, 0.87, 0.66], # journey   (x^2)\n",
    "    [0.57, 0.85, 0.64], # starts    (x^3)\n",
    "    [0.22, 0.58, 0.33], # with      (x^4)\n",
    "    [0.77, 0.25, 0.10], # one       (x^5)\n",
    "    [0.05, 0.80, 0.55]  # step      (x^6)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f1698a87-4389-4420-aaa6-0ca7195fa242",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttentionV1(nn.Module):\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.Wq = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.Wk = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.Wv = nn.Parameter(torch.rand(d_in, d_out))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        queries = x @ self.Wq\n",
    "        keys = x @ self.Wk\n",
    "        values = x @ self.Wv\n",
    "        \n",
    "        attn_scores = queries @ keys.T\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        context_vec = attn_weights @ values\n",
    "        \n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7ff6b71d-cc25-4ea3-a64d-aa11354ce4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_in = inputs.shape[1]\n",
    "d_out = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2a9708a8-a6b2-4655-9ece-63db8d029dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(666)\n",
    "\n",
    "self_attn_v1 = SelfAttentionV1(d_in, d_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ce956bb6-49e2-43e0-9167-2d2ad91d5b16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9859, 1.0707],\n",
      "        [0.9891, 1.0748],\n",
      "        [0.9892, 1.0749],\n",
      "        [0.9825, 1.0657],\n",
      "        [0.9863, 1.0707],\n",
      "        [0.9825, 1.0659]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(self_attn_v1(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "56dfc6df-ff6a-4737-9f18-bbdae052b7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttentionV2(nn.Module):\n",
    "    def __init__(self, d_in, d_out, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.Wq = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.Wk = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.Wv = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        queries = self.Wq(x)\n",
    "        keys = self.Wk(x)\n",
    "        values = self.Wv(x)\n",
    "        \n",
    "        attn_scores = queries @ keys.T\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        context_vec = attn_weights @ values\n",
    "        \n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "507a81ad-3ffb-4736-810d-2230fce62f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "self_attn_v2 = SelfAttentionV2(d_in, d_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "dac8a68a-14ca-47eb-91a3-04c908722666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.5231,  0.2215],\n",
      "        [-0.5259,  0.2240],\n",
      "        [-0.5260,  0.2239],\n",
      "        [-0.5282,  0.2260],\n",
      "        [-0.5281,  0.2231],\n",
      "        [-0.5274,  0.2267]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(self_attn_v2(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1b619911-f00d-489d-a61e-9c05464f1c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3.1 - Comparing SelfAttention_v1 and SelfAttention_v2\n",
    "\n",
    "sa_v1 = SelfAttentionV1(d_in, d_out)\n",
    "sa_v1.Wq = nn.Parameter(self_attn_v2.Wq.weight.T)\n",
    "sa_v1.Wk = nn.Parameter(self_attn_v2.Wk.weight.T)\n",
    "sa_v1.Wv = nn.Parameter(self_attn_v2.Wv.weight.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1c5bd745-a79c-4a69-958f-0feeda3fbfcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SelfAttentionV1 Context Vectors:\n",
      "\n",
      " tensor([[-0.5231,  0.2215],\n",
      "        [-0.5259,  0.2240],\n",
      "        [-0.5260,  0.2239],\n",
      "        [-0.5282,  0.2260],\n",
      "        [-0.5281,  0.2231],\n",
      "        [-0.5274,  0.2267]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(\"SelfAttentionV1 Context Vectors:\\n\\n\", sa_v1(inputs))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
