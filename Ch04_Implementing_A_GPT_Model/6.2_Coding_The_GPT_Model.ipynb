{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f62677f-5378-4d95-ac42-79ff0397ccff",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%run 6.1_Coding_The_GPT_Model.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4f33a81-6f37-44cc-aa62-b1c0c5149108",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 4.1 Number of Parameters in Feed Forward and MHA Modules\n",
    "\n",
    "trf_block = TransformerBlock(GPT_CONFIG_124M)\n",
    "\n",
    "ffn_params = sum(p.numel() for p in trf_block.ff.parameters())\n",
    "ffn_params_total = sum(p.numel() for p in block.ff.parameters()) * 12\n",
    "\n",
    "mha_params = sum(p.numel() for p in block.att.parameters())\n",
    "mha_params_total = sum(p.numel() for p in block.att.parameters()) * 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1446929b-0d4e-43c5-8f55-f21909aebb9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01 Feed forward Module Parameters: 4,722,432\n",
      "12 Feed forward Module Parameters: 56,669,184\n"
     ]
    }
   ],
   "source": [
    "print(f\"01 Feed forward Module Parameters: {ffn_params:,}\")\n",
    "print(f\"12 Feed forward Module Parameters: {ffn_params_total:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b874d4e-e868-4e2d-8aad-29986b1ed327",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01 Multi-Head Atttention Module Parameters: 2,360,064\n",
      "12 Multi-Head Atttention Module Parameters: 28,320,768\n"
     ]
    }
   ],
   "source": [
    "print(f\"01 Multi-Head Atttention Module Parameters: {mha_params:,}\")\n",
    "print(f\"12 Multi-Head Atttention Module Parameters: {mha_params_total:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab729af2-c26c-4684-8c41-1a7393b26371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 4.2 Initializing larger GPT models\n",
    "\n",
    "GPT_CONFIG = {\n",
    "    \"vocab_size\": 50257,        # Vocabulary Size\n",
    "    \"context_length\": 1024,     # Context Length\n",
    "    \"emb_dim\": 768,             # Embedding Dimension\n",
    "    \"n_heads\": 12,              # Number of Attention Heads\n",
    "    \"n_layers\": 12,             # Number of Transformer Blocks\n",
    "    \"drop_rate\": 0.1,           # Dropout Rate\n",
    "    \"qkv_bias\": False           # Query-Key-Value bias\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5732b9ef-00cc-4bd5-b9b8-9cf0e9f35855",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_config(base_config, model_name=\"gpt2-small\"):\n",
    "    GPT_CONFIG = base_config.copy()\n",
    "\n",
    "    if model_name == \"gpt2-small\":\n",
    "        GPT_CONFIG[\"emb_dim\"] = 768\n",
    "        GPT_CONFIG[\"n_layers\"] = 12\n",
    "        GPT_CONFIG[\"n_heads\"] = 12\n",
    "    \n",
    "    elif model_name == \"gpt2-medium\":\n",
    "        GPT_CONFIG[\"emb_dim\"] = 1024\n",
    "        GPT_CONFIG[\"n_layers\"] = 24\n",
    "        GPT_CONFIG[\"n_heads\"] = 16\n",
    "    \n",
    "    elif model_name == \"gpt2-large\":\n",
    "        GPT_CONFIG[\"emb_dim\"] = 1280\n",
    "        GPT_CONFIG[\"n_layers\"] = 36\n",
    "        GPT_CONFIG[\"n_heads\"] = 20\n",
    "    \n",
    "    elif model_name == \"gpt2-xl\":\n",
    "        GPT_CONFIG[\"emb_dim\"] = 1600\n",
    "        GPT_CONFIG[\"n_layers\"] = 48\n",
    "        GPT_CONFIG[\"n_heads\"] = 25\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Incorrect Model Name {model_name}\")\n",
    "    \n",
    "    return GPT_CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "603ef50d-8de4-416a-8116-4d0278452f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_size(model):\n",
    "\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    \n",
    "    # Total Parameters using Weight Tying\n",
    "    total_params_gpt2 = total_params - sum(p.numel() for p in model.out_head.parameters())\n",
    "    print(f\"Number of Trainable Parameters using Weight Tying => {total_params_gpt2:,}\")\n",
    "    \n",
    "    # Total size in Bytes (assuming float32, 4 bytes per parameter)\n",
    "    total_size_bytes = total_params * 4\n",
    "    \n",
    "    # Convert to Megabytes\n",
    "    total_size_mb = total_size_bytes / (1024 * 1024)\n",
    "\n",
    "    print(f\"Total Size of the Model: {total_size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1836ffe-7a07-48a9-80fa-83d2e7dcc963",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt2-large:\n",
      "Number of Trainable Parameters using Weight Tying => 773,891,840\n",
      "Total Size of the Model: 3197.56 MB\n"
     ]
    }
   ],
   "source": [
    "model_sizes = [\"small\", \"medium\", \"large\", \"xl\"]\n",
    "\n",
    "model_name = f\"gpt2-{model_sizes[2]}\"\n",
    "CONFIG = get_config(GPT_CONFIG, model_name=model_name)\n",
    "\n",
    "model = GPTModel(CONFIG)\n",
    "print(f\"{model_name}:\")\n",
    "calculate_size(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
