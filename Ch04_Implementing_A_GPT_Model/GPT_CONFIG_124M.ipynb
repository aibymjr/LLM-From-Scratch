{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc11e783-5298-4d04-8c35-8b639ff1f50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,\n",
    "    \"context_length\": 1024,\n",
    "    \"emb_dim\": 768,\n",
    "    \"n_heads\": 12,\n",
    "    \"n_layers\": 12,\n",
    "    \"drop_rate\": 0.1,\n",
    "    \"qkv_bias\": False\n",
    "}\n",
    "\n",
    "# vocab_size: refers to a vocabulary of 50,257 words, as used by the BPE tokenizer (see chapter 2).\n",
    "\n",
    "# context_length: denotes the maximum number of input tokens the model can handle via the positional embeddings (see chapter 2).\n",
    "\n",
    "# emb_dim: represents the embedding size, transforming each token into a 768- dimensional vector.\n",
    "\n",
    "# n_heads: indicates the count of attention heads in the multi-head attention mechanism (see chapter 3).\n",
    "\n",
    "# n_layers: specifies the number of transformer blocks in the model, which we will cover in the upcoming discussion.\n",
    "\n",
    "# drop_rate: indicates the intensity of the dropout mechanism (0.1 implies a 10% random drop out of hidden units) to prevent overfitting (see chapter 3).\n",
    "\n",
    "# qkv_bias: determines whether to include a bias vector in the Linear layers of the multi-head attention for query, key, and value computations. \n",
    "    # We will initially disable this, following the norms of modern LLMs, but we will revisit it in chapter 6 when we load pretrained GPT-2 weights \n",
    "    # from OpenAI into our model (see chapter 6)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
